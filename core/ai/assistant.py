"""
AIåŠ©æ‰‹æœåŠ¡ - æä¾›æ™ºèƒ½å¯¹è¯å’Œåˆ†æåŠŸèƒ½
"""

import asyncio
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import logging

from .openai_client import OpenAIClient, get_openai_client, is_openai_available
from .config import get_ai_config, AIConfig

logger = logging.getLogger(__name__)

class AIAssistant:
    """AIåŠ©æ‰‹æœåŠ¡ç±»"""

    def __init__(self, config: Optional[AIConfig] = None):
        """
        åˆå§‹åŒ–AIåŠ©æ‰‹

        Args:
            config: AIé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or get_ai_config()
        self.openai_client = get_openai_client() if self.config.is_valid() else None
        self.request_count = 0
        self.last_reset_date = datetime.now().date()

        logger.info(f"AIåŠ©æ‰‹åˆå§‹åŒ–å®Œæˆï¼ŒOpenAIå¯ç”¨: {self.is_available()}")

    def is_available(self) -> bool:
        """æ£€æŸ¥AIåŠ©æ‰‹æ˜¯å¦å¯ç”¨"""
        return self.openai_client is not None and self._check_rate_limit()

    def _check_rate_limit(self) -> bool:
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
        current_date = datetime.now().date()

        # å¦‚æœæ˜¯æ–°çš„ä¸€å¤©ï¼Œé‡ç½®è®¡æ•°å™¨
        if current_date != self.last_reset_date:
            self.request_count = 0
            self.last_reset_date = current_date

        return self.request_count < self.config.max_daily_requests

    def _increment_request_count(self):
        """å¢åŠ è¯·æ±‚è®¡æ•°"""
        self.request_count += 1

    async def chat(
        self,
        user_message: str,
        user_context: Dict[str, Any],
        conversation_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        ä¸AIåŠ©æ‰‹å¯¹è¯

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡æ•°æ®
            conversation_history: å¯¹è¯å†å²

        Returns:
            AIå“åº”ç»“æœ
        """
        if not self.is_available():
            return self._get_unavailable_response(user_message, user_context)

        try:
            self._increment_request_count()

            # å¤„ç†å¯¹è¯å†å²
            if conversation_history and self.config.enable_conversation_memory:
                # é™åˆ¶å†å²é•¿åº¦
                conversation_history = conversation_history[-self.config.max_conversation_history:]

            # è°ƒç”¨OpenAIå®¢æˆ·ç«¯
            response = await self.openai_client.get_ai_response(
                user_message=user_message,
                user_context=user_context,
                conversation_history=conversation_history
            )

            # è®°å½•æˆåŠŸå“åº”
            logger.info(f"AIå“åº”æˆåŠŸï¼Œä½¿ç”¨tokens: {response.get('tokens_used', 0)}")

            return response

        except Exception as e:
            logger.error(f"AIå¯¹è¯å¤±è´¥: {str(e)}")
            return self._get_error_response(user_message, user_context, str(e))

    def chat_sync(
        self,
        user_message: str,
        user_context: Dict[str, Any],
        conversation_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """åŒæ­¥ç‰ˆæœ¬çš„å¯¹è¯æ¥å£"""
        if not self.is_available():
            return self._get_unavailable_response(user_message, user_context)

        try:
            self._increment_request_count()

            # å¤„ç†å¯¹è¯å†å²
            if conversation_history and self.config.enable_conversation_memory:
                conversation_history = conversation_history[-self.config.max_conversation_history:]

            # è°ƒç”¨OpenAIå®¢æˆ·ç«¯ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            response = self.openai_client.get_ai_response_sync(
                user_message=user_message,
                user_context=user_context,
                conversation_history=conversation_history
            )

            logger.info(f"AIå“åº”æˆåŠŸï¼Œä½¿ç”¨tokens: {response.get('tokens_used', 0)}")
            return response

        except Exception as e:
            logger.error(f"AIå¯¹è¯å¤±è´¥: {str(e)}")
            return self._get_error_response(user_message, user_context, str(e))

    async def generate_insights(self, user_context: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ

        Args:
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡æ•°æ®

        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        if not self.is_available() or not self.config.enable_insights_generation:
            return self._get_default_insights(user_context)

        try:
            self._increment_request_count()

            # è°ƒç”¨OpenAIç”Ÿæˆæ´å¯Ÿ
            insights = await self.openai_client.generate_insights(user_context)

            logger.info(f"æ™ºèƒ½æ´å¯Ÿç”ŸæˆæˆåŠŸï¼Œå…±{len(insights)}æ¡")
            return insights

        except Exception as e:
            logger.error(f"æ™ºèƒ½æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._get_default_insights(user_context)

    def _get_unavailable_response(self, user_message: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–AIä¸å¯ç”¨æ—¶çš„å“åº”"""
        if not self.config.enable_fallback_responses:
            return {
                "response": "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "intent": "service_unavailable",
                "confidence": 0.0,
                "model": "unavailable",
                "timestamp": datetime.now().isoformat()
            }

        # ä½¿ç”¨é™çº§å“åº”
        try:
            import sys
            from pathlib import Path
            project_root = Path(__file__).parent.parent.parent
            sys.path.insert(0, str(project_root))

            from ui.components.chat import get_ai_response_mock
            fallback_response = get_ai_response_mock(user_message, user_context)

            return {
                "response": fallback_response,
                "intent": "fallback_response",
                "confidence": 0.6,
                "model": "fallback",
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            return {
                "response": "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æ•°æ®æ¦‚è§ˆé¡µé¢äº†è§£è®¢é˜…æƒ…å†µã€‚",
                "intent": "error_fallback",
                "confidence": 0.3,
                "error": str(e),
                "model": "error",
                "timestamp": datetime.now().isoformat()
            }

    def _get_error_response(self, user_message: str, user_context: Dict[str, Any], error: str) -> Dict[str, Any]:
        """è·å–é”™è¯¯å“åº”"""
        return {
            "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚",
            "intent": "error",
            "confidence": 0.0,
            "error": error,
            "model": "error",
            "timestamp": datetime.now().isoformat()
        }

    def _get_default_insights(self, user_context: Dict[str, Any]) -> List[Dict[str, str]]:
        """ç”Ÿæˆé»˜è®¤æ´å¯Ÿ"""
        insights = []

        monthly_spending = user_context.get("monthly_spending", 0)
        subscriptions = user_context.get("subscriptions", [])
        categories = user_context.get("subscription_categories", {})

        # æ”¯å‡ºåˆ†æ
        if monthly_spending > 200:
            insights.append({
                "type": "warning",
                "icon": "âš ï¸",
                "title": "æ”¯å‡ºè¾ƒé«˜æé†’",
                "content": f"æ‚¨çš„æœˆåº¦è®¢é˜…æ”¯å‡ºä¸ºÂ¥{monthly_spending:.2f}ï¼Œå»ºè®®å®šæœŸè¯„ä¼°å„æœåŠ¡çš„ä½¿ç”¨é¢‘ç‡ã€‚"
            })

        # è®¢é˜…æ•°é‡åˆ†æ
        if len(subscriptions) > 5:
            insights.append({
                "type": "info",
                "icon": "ğŸ“±",
                "title": "è®¢é˜…æ•°é‡æé†’",
                "content": f"æ‚¨æœ‰{len(subscriptions)}ä¸ªæ´»è·ƒè®¢é˜…ï¼Œå¯ä»¥è€ƒè™‘æ•´åˆç›¸ä¼¼åŠŸèƒ½çš„æœåŠ¡ã€‚"
            })

        # å¨±ä¹æ”¯å‡ºåˆ†æ
        entertainment_cost = categories.get("entertainment", {}).get("spending", 0)
        if entertainment_cost > 50:
            insights.append({
                "type": "info",
                "icon": "ğŸ¬",
                "title": "å¨±ä¹æ”¯å‡ºåˆ†æ",
                "content": f"å¨±ä¹ç±»æ”¯å‡ºÂ¥{entertainment_cost:.2f}/æœˆï¼Œå¯ä»¥è€ƒè™‘é€‰æ‹©æ€§ä¿ç•™æœ€å¸¸ç”¨çš„æœåŠ¡ã€‚"
            })

        # å¦‚æœæ²¡æœ‰ç‰¹æ®Šæƒ…å†µï¼Œç»™å‡ºç§¯æåé¦ˆ
        if not insights:
            insights.append({
                "type": "success",
                "icon": "âœ…",
                "title": "è®¢é˜…ç»“æ„è‰¯å¥½",
                "content": "æ‚¨çš„è®¢é˜…ç®¡ç†æƒ…å†µå¾ˆä¸é”™ï¼Œç»§ç»­ä¿æŒå®šæœŸè¯„ä¼°çš„ä¹ æƒ¯ï¼"
            })

        return insights

    def get_status(self) -> Dict[str, Any]:
        """è·å–AIåŠ©æ‰‹çŠ¶æ€ä¿¡æ¯"""
        return {
            "available": self.is_available(),
            "openai_configured": self.openai_client is not None,
            "model": self.config.openai_model,
            "daily_requests_used": self.request_count,
            "daily_limit": self.config.max_daily_requests,
            "features": {
                "insights_generation": self.config.enable_insights_generation,
                "conversation_memory": self.config.enable_conversation_memory,
                "fallback_responses": self.config.enable_fallback_responses
            },
            "config": self.config.to_dict()
        }

    def reset_daily_limit(self):
        """é‡ç½®æ¯æ—¥é™åˆ¶ï¼ˆæµ‹è¯•ç”¨ï¼‰"""
        self.request_count = 0
        self.last_reset_date = datetime.now().date()

# å…¨å±€åŠ©æ‰‹å®ä¾‹
_ai_assistant = None

def get_ai_assistant() -> AIAssistant:
    """è·å–AIåŠ©æ‰‹å®ä¾‹"""
    global _ai_assistant

    if _ai_assistant is None:
        _ai_assistant = AIAssistant()

    return _ai_assistant

def is_ai_assistant_available() -> bool:
    """æ£€æŸ¥AIåŠ©æ‰‹æ˜¯å¦å¯ç”¨"""
    assistant = get_ai_assistant()
    return assistant.is_available()

async def chat_with_ai(
    user_message: str,
    user_context: Dict[str, Any],
    conversation_history: Optional[List[Dict]] = None
) -> Dict[str, Any]:
    """ä¾¿æ·çš„AIå¯¹è¯æ¥å£"""
    assistant = get_ai_assistant()
    return await assistant.chat(user_message, user_context, conversation_history)

def chat_with_ai_sync(
    user_message: str,
    user_context: Dict[str, Any],
    conversation_history: Optional[List[Dict]] = None
) -> Dict[str, Any]:
    """ä¾¿æ·çš„AIå¯¹è¯æ¥å£ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    assistant = get_ai_assistant()
    return assistant.chat_sync(user_message, user_context, conversation_history)

async def generate_ai_insights(user_context: Dict[str, Any]) -> List[Dict[str, str]]:
    """ä¾¿æ·çš„æ™ºèƒ½æ´å¯Ÿç”Ÿæˆæ¥å£"""
    assistant = get_ai_assistant()
    return await assistant.generate_insights(user_context)